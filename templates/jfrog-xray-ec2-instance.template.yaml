AWSTemplateFormatVersion: "2010-09-09"
Description: "Xray: Deploys the EC2 Autoscaling, LaunchConfig and Instances (qs-1rtidak0g)"
Metadata:
  cfn-lint:
    config:
      ignore_checks:
        - W9006
        - W9002
        - W9003
        - W9004
        - E9101
      ignore_reasons:
        - E9101: "'master' is part of the product naming conventions for now"
Parameters:
  PrivateSubnetIds:
    Type: List<AWS::EC2::Subnet::Id>
  # PrivateSubnet1Id:
  #   Type: AWS::EC2::Subnet::Id
  # PrivateSubnet2Id:
  #   Type: AWS::EC2::Subnet::Id
  # KeyPairName:
  #   Type: AWS::EC2::KeyPair::KeyName
  MinScalingNodes:
    Type: Number
  MaxScalingNodes:
    Type: Number
  DeploymentTag:
    Type: String
  S3BucketName:
    Type: String
  # QsS3BucketName:
  #   Type: String
  # QsS3KeyPrefix:
  #   Type: String
  # QsS3Uri:
  #   Type: String
  DatabaseDriver:
    Type: String
  DatabaseType:
    Type: String
  DatabaseUser:
    Type: String
  DatabasePassword:
    Type: String
    NoEcho: 'true'
  MasterKey:
    Type: String
    NoEcho: 'true'
  ExtraJavaOptions:
    Type: String
  SecurityGroups:
    Type: String
  XrayHostProfile:
    Type: String
  XrayHostRole:
    Type: String
  XrayInstanceType:
    Type: String
  JfrogInternalUrl:
    Type: String
  VolumeSize:
    Type: Number
  XrayDatabaseUser:
    Type: String
  XrayDatabasePassword:
    Type: String
    NoEcho: 'true'
  XrayMasterDatabaseUrl:
    Type: String
  XrayDatabaseUrl:
    Type: String
  XrayVersion:
    Type: String

# To populate additional mappings use following link
# https://raw.githubusercontent.com/aws-quickstart/quickstart-linux-bastion/master/templates/linux-bastion.template
# link from centos: https://www.centos.org/download/aws-images/
Mappings:
  AWSAMIRegionMap:
    us-west-2:
      CentOS7HVM: "ami-0bc06212a56393ee1"
      RockyLinux8LVM: "ami-02dd44a54586c69fb"

Resources:
  XrayScalingGroup:
    Type: AWS::AutoScaling::AutoScalingGroup
    Properties:
      LaunchConfigurationName: !Ref XrayLaunchConfiguration
      VPCZoneIdentifier: !Ref PrivateSubnetIds
        # - !Ref PrivateSubnet1Id
        # - !Ref PrivateSubnet2Id
      MinSize: !Ref MinScalingNodes
      MaxSize: !Ref MaxScalingNodes
      Cooldown: '300'
      DesiredCapacity: !Ref MinScalingNodes
      HealthCheckType: EC2
      HealthCheckGracePeriod: 1800
      Tags:
        - Key: Name
          Value: !Ref DeploymentTag
          PropagateAtLaunch: true
        - Key: XrayVersion
          Value: !Ref XrayVersion
          PropagateAtLaunch: true
      TerminationPolicies:
        - OldestInstance
        - Default
    CreationPolicy:
      ResourceSignal:
        Count: !Ref MinScalingNodes
        Timeout: PT60M
  XrayLaunchConfiguration:
    Type: AWS::AutoScaling::LaunchConfiguration
    Metadata:
      AWS::CloudFormation::Authentication:
        S3AccessCreds:
          type: S3
          roleName:
            - !Ref XrayHostRole
          buckets:
            - !Ref S3BucketName
            # - !Ref QsS3BucketName
      AWS::CloudFormation::Init:
        configSets:
          xray_ami_setup:
            - "cfn_hup"
            - "config-cloudwatch"
            - "config-ansible-xray-ami"
          xray_install:
            - "cfn_hup"
            - "config-cloudwatch"
            - "config-ansible-xray-ami"
            - "config-xray"
            - "secure-xray"
          reconfigure:
            - "config-xray"
            - "secure-xray"
        cfn_hup:
          commands:
            1_config_dirs:
              command: mkdir -p /etc/cfn/hooks.d
              test: test ! -d /etc/cfn/hooks.d
          files:
            /etc/cfn/cfn-hup.conf:
              content: !Sub |
                [main]
                stack=${AWS::StackName}
                region=${AWS::Region}
            /etc/cfn/hooks.d/cfn-reconfigure.conf:
              content: !Sub |
                [cfn-reconfigure-hook]
                triggers=post.update
                path=Resources.Ec2Instance.Metadata.AWS::CloudFormation::Init
                action=/usr/local/bin/cfn-init -v --stack ${AWS::StackName} --resource XrayScalingGroup --configsets reconfigure --region ${AWS::Region}
                runas=root
              mode: "000400"
              owner: "root"
              group: "root"
          services:
            sysvinit:
              cfn-hup:
                enabled: true
                ensureRunning: true
                files:
                  - /etc/cfn/cfn-hup.conf
                  - /etc/cfn/hooks.d/cfn-reconfigure.conf

        config-cloudwatch:
          files:
            /root/cloudwatch.conf:
              content: !Sub |
                [general]
                state_file = /var/awslogs/state/agent-state

                [var_log_messages]
                file = /var/log/messages
                log_group_name = /artifactory/instances/{instance_id}
                log_stream_name = var_log_messages
                datetime_format = %b %d %H:%M:%S

                [xray_installation]
                file = /var/log/xray*
                log_group_name = /artifactory/instances/{instance_id}
                log_stream_name = xray_installation
                datetime_format = %b %d %H:%M:%S

                [xray_logs]
                file = /opt/jfrog/jfrog-xray-${XrayVersion}-linux/var/log/*.log
                log_group_name = /artifactory/instances/{instance_id}
                log_stream_name = xray_logs
                datetime_format = %b %d %H:%M:%S
              mode: "0400"
        config-ansible-xray-ami:
          files:
            /root/.xray_ami/xray-ami-setup.yml:
              content: !Sub |
                  # Base install for Xray
                  - import_playbook: xray-ami.yml
                    vars:
                      ami_creation: false
                      db_type: postgresql
                      db_driver: org.postgresql.Driver
                      xray_version: ${XrayVersion}
                      xray_ha_enabled: false
              mode: "0400"
        config-xray:
          files:
            /root/create_rabbitmq_cluster.sh:
              content: !Sub |
                #!/usr/bin/env bash
                export HOME=/root
                cd
                cp ~xray/.erlang.cookie /root/
                curl http://localhost:15672/cli/rabbitmqadmin > ./rabbitmqadmin
                chmod 755 ./rabbitmqadmin
                date
                # Add a random wait up to a max of 300 secs, so all servers start at different times.
                # this helps all rabbitmq nodes to detect the same cluster
                # without this sleep, rabbit
                sleep $(( ( RANDOM % 300 )  + 100 ))
                date
                curl -s http://localhost:8046/router/api/v1/topology/health
                OTHERHOST=$(curl -s http://localhost:8046/router/api/v1/topology/health | jq -r '.nodes | keys[] as $k | "\($k), \(.[$k].effective_state), \(.[$k].health_response.services[].service_id)"' | grep -v UNHEALTHY | grep jfx | grep -v $(curl http://169.254.169.254/latest/meta-data/local-ipv4) | cut -d ',' -f 1-1 | uniq | head -1 | sed "s/.*http:\/\/\(.*\):8082.*/\1/")
                echo "OTHERHOST = $OTHERHOST"
                CLUSTER=$(./rabbitmqadmin -H $OTHERHOST list nodes | grep rabbit | head -1 | cut -s -d ' ' -f 2-2)
                echo "CLUSTER = $CLUSTER"
                if [ -z "$CLUSTER" ]
                then
                  echo "No cluster found. Treating this node as first node"
                  # no other node is found. So this must be first node.
                  # no need to connect to cluster, let it form its own cluster
                else
                  /opt/jfrog/jfrog-xray*/app/third-party/rabbitmq/sbin/rabbitmqctl stop_app
                  /opt/jfrog/jfrog-xray*/app/third-party/rabbitmq/sbin/rabbitmqctl force_reset
                  /opt/jfrog/jfrog-xray*/app/third-party/rabbitmq/sbin/rabbitmqctl join_cluster $CLUSTER
                fi
                /opt/jfrog/jfrog-xray*/app/third-party/rabbitmq/sbin/rabbitmqctl start_app
                /opt/jfrog/jfrog-xray*/app/third-party/rabbitmq/sbin/rabbitmqctl cluster_status
              mode: "0770"
            /root/.xray_ami/xray.yml:
              content: !Sub |
                # Base install for Xray
                - import_playbook: site-xray.yml
                  vars:
                    jfrog_url: ${JfrogInternalUrl}
                    master_key: ${MasterKey}
                    join_key: ${MasterKey}
                    extra_java_opts: ${ExtraJavaOptions}
                    db_type: ${DatabaseType}
                    db_driver: ${DatabaseDriver}
                    db_master_url: postgresql://${DatabaseUser}:${DatabasePassword}@${XrayMasterDatabaseUrl}
                    db_url: postgres://${XrayDatabaseUrl}
                    db_master_user: ${DatabaseUser}
                    db_user: ${XrayDatabaseUser}
                    db_password: ${XrayDatabasePassword}
                    xray_version: ${XrayVersion}
              mode: "0400"
            /root/.vault_pass.txt:
              content: !Sub |
                ${DatabasePassword}
              mode: "0400"
            /root/.secureit.sh:
              content:
                ansible-vault encrypt /root/.xray_ami/xray.yml --vault-id /root/.vault_pass.txt
              mode: "0770"
        secure-xray:
          commands:
            'secure ansible playbook':
              command: '/root/.secureit.sh'
              ignoreErrors: 'false'
    Properties:
      # KeyName: !Ref KeyPairName
      IamInstanceProfile: !Ref XrayHostProfile
      ImageId: !FindInMap
        - AWSAMIRegionMap
        - !Ref AWS::Region
        - 'RockyLinux8LVM'
      SecurityGroups:
        - !Ref SecurityGroups
      InstanceType: !Ref XrayInstanceType
      BlockDeviceMappings:
        - DeviceName: /dev/sda1
          Ebs:
            VolumeSize: !Ref VolumeSize
            VolumeType: gp2
            DeleteOnTermination: true
            Encrypted: true
      UserData:
        Fn::Base64:
          !Sub |
            #!/bin/bash -x

            #exec > >(tee /var/log/user-data.log|logger -t user-data -s 2>/dev/console) 2>&1

            #CFN Functions

            function cfn_fail
            {
              cfn-signal -e 1 --stack ${AWS::StackName} --region ${AWS::Region} --resource XrayScalingGroup
              exit 1
            }

            function cfn_success
            {
              cfn-signal -e 0 --stack ${AWS::StackName} --region ${AWS::Region} --resource XrayScalingGroup
              exit 0
            }

            # Installation functions

            function install_cfn_centos7
            {
              if [ ! -x /etc/init.d/cfn-hup ]; then
                echo "Install CloudFormation::Init tools"
                curl -o aws-cfn.tgz https://s3.amazonaws.com/cloudformation-examples/aws-cfn-bootstrap-latest.tar.gz
                tar -xzf aws-cfn.tgz
                cd aws-cfn-bootstrap-[0-9]*
                yum install -y pystache python-daemon python-requests python2-configargparse
                python ./setup.py build
                python ./setup.py install
                chmod +x /usr/init/redhat/cfn-hup
                ln -s /usr/init/redhat/cfn-hup /etc/init.d/cfn-hup
                sed -i -e 's!daemon /opt/aws!daemon /usr!' /etc/init.d/cfn-hup
                chkconfig cfn-hup on
                cd ..
                rm -rf aws-cfn*
              fi
            }

            function install_ssm_agent
            {
              yum install -y https://s3.${AWS::Region}.amazonaws.com/amazon-ssm-${AWS::Region}/latest/linux_amd64/amazon-ssm-agent.rpm
              systemctl enable --now amazon-ssm-agent
              systemctl status amazon-ssm-agent
            }

            function install_awscli
            {
              if [ ! -d /usr/local/aws-cli ]; then
                curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o awscliv2.zip
                unzip -q awscliv2.zip
                ./aws/install -i /usr/local/aws-cli -b /usr/local/bin
                rm -rf aws*
              fi
            }
            
            function selinux_logrotate_services
            {
            # Allow logrotate to restart services, some rotation policies may requite it. 
                echo '(allow logrotate_t etc_t (service (start)))' > /root/logrotate_services.cil
                semodule -i /root/logrotate_services.cil
            }

            function setup_cloud_watch
            {
              curl https://s3.amazonaws.com/aws-cloudwatch/downloads/latest/awslogs-agent-setup.py -O
              chmod +x ./awslogs-agent-setup.py
              ./awslogs-agent-setup.py -n -r ${AWS::Region} -c /root/cloudwatch.conf
              systemctl enable awslogs
            }

            function update_os
            {
              # Update YUM repositories to use vault.centos.org for CentOS 7.7.1908
              cat > /etc/yum.repos.d/CentOS-Base.repo << EOF
            [base]
            name=CentOS-7.7.1908 - Base
            baseurl=http://vault.centos.org/7.7.1908/os/x86_64/
            gpgcheck=1
            gpgkey=http://vault.centos.org/7.7.1908/os/x86_64/RPM-GPG-KEY-CentOS-7
            
            [updates]
            name=CentOS-7.7.1908 - Updates
            baseurl=http://vault.centos.org/7.7.1908/updates/x86_64/
            gpgcheck=1
            gpgkey=http://vault.centos.org/7.7.1908/os/x86_64/RPM-GPG-KEY-CentOS-7
            
            [extras]
            name=CentOS-7.7.1908 - Extras
            baseurl=http://vault.centos.org/7.7.1908/extras/x86_64/
            gpgcheck=1
            gpgkey=http://vault.centos.org/7.7.1908/os/x86_64/RPM-GPG-KEY-CentOS-7
            EOF

              yum clean all
              yum update -y
              yum install -y epel-release
              yum update -y
              yum install -y ca-certificates
              update-ca-trust force-enable
              update-ca-trust extract
              # CentOS cloned virtual machines do not create a new machine id
              # https://www.thegeekdiary.com/centos-rhel-7-how-to-change-the-machine-id/
              rm -f /etc/machine-id
              systemd-machine-id-setup
            }

            function install_python3
            {
              yum install -y python3 libselinux-python3 python3-wheel
            }

            function setup_local_path
            {
              echo 'export PATH=$HOME/.local/bin:/usr/local/bin:$PATH' >> ~/.bashrc
              . ~/.bashrc
            }

            function install_required_packages
            {
              yum install -y git jq nfs-utils policycoreutils-python unzip ansible
              ansible-galaxy collection install community.general ansible.posix --ignore-certs
              yum install -y postgresql-server postgresql-devel
              yum update -y --security
            }

            # Run install tasks

            # resize root volume
            xfs_growfs /dev/nvme0n1p1

            update_os
            selinux_logrotate_services
            install_ssm_agent
            install_required_packages
            install_python3
            setup_local_path
            install_cfn_centos7
            install_awscli

            mkdir ~/.xray_ami
            aws s3 --region ${AWS::Region} sync s3://${S3BucketName}/cloudInstallerScripts/ ~/.xray_ami/

            setsebool httpd_can_network_connect 1 -P

            cfn-init -v --stack ${AWS::StackName} --resource XrayLaunchConfiguration --configsets xray_install --region ${AWS::Region} || cfn_fail

            setup_cloud_watch

            ansible-playbook /root/.xray_ami/xray-ami-setup.yml --vault-id /root/.vault_pass.txt 2>&1 | tee /var/log/xray-ami.log || cfn_fail
            ansible-playbook /root/.xray_ami/xray.yml --vault-id /root/.vault_pass.txt 2>&1 | tee /var/log/xray.log || cfn_fail

            rm -rf /root/.secureit.sh

            cfn_success &> /var/log/cfn_success.log
            cfn_success || cfn_fail
